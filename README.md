# Fine-tuning Instruction-Following Pre-trained Models for Text Summarization

## Overview

This project involves fine-tuning a pre-trained instruction-following language model, such as Llama, Mistral, or Gemma, to perform the task of text summarization. The objective is to achieve a model capable of generating concise and relevant summaries from given input texts based on specific instructions.

## Example Prompt: Generate a brief summary of the meeting within 300 characters

## Dataset

The dataset used for this task is the RussianNLP/Mixed-Summarization-Dataset. This dataset contains a variety of text samples and their corresponding summaries, making it ideal for training a summarization model.
